{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sahil\n",
      "[nltk_data]     Prusti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sahil\n",
      "[nltk_data]     Prusti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: prustisahil (prustisahil-penn-state) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Sahil Prusti\\OneDrive - The Pennsylvania State University\\Desktop\\340 Project\\wandb\\run-20250407_182310-47ru840h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prustisahil-penn-state/mental_health_multilabel/runs/47ru840h' target=\"_blank\">swift-plasma-2</a></strong> to <a href='https://wandb.ai/prustisahil-penn-state/mental_health_multilabel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prustisahil-penn-state/mental_health_multilabel' target=\"_blank\">https://wandb.ai/prustisahil-penn-state/mental_health_multilabel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prustisahil-penn-state/mental_health_multilabel/runs/47ru840h' target=\"_blank\">https://wandb.ai/prustisahil-penn-state/mental_health_multilabel/runs/47ru840h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Setup and Imports\n",
    "# \n",
    "# We import all required libraries. This includes data manipulation, aggressive text preprocessing,\n",
    "# Hugging Face Transformers for RoBERTa, PyTorch for model training, scikit-learn for metrics,\n",
    "# and WandB for logging.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "# Aggressive text cleaning may require emoji support\n",
    "import emoji\n",
    "\n",
    "# For tokenization and normalization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Hugging Face Transformers for RoBERTa\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn for multilabel metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# WandB for logging (optional but recommended)\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"mental_health_multilabel\", config={\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 2e-5,\n",
    "    \"max_length\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "})\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with shape: (365582, 2)\n",
      "Sample cleaned text:\n",
      "0    is hiring a life coach worth the money i am a ...\n",
      "1    i dont know whether im clinically depressed or...\n",
      "2    i always have big plans and ideas but i fuck u...\n",
      "3    why is everything going wrong i go to a boardi...\n",
      "4    cant tell if im depressed thanks in advance to...\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Data Loading and Aggressive Text Preprocessing (“Wading through Reddit soup”)\n",
    "# \n",
    "# We load the CSV file (\"mentalhealth.csv\") and preprocess the text.\n",
    "# Our cleaning function aggressively removes emojis, emoticons, Reddit markdown (e.g., quotes, spoilers),\n",
    "# and emotionally ambiguous punctuation. It also normalizes common slang/abbreviations using a simple mapping.\n",
    "# We take care not to over-clean so that the “soul” (context) of the post is preserved.\n",
    "\n",
    "# %%\n",
    "# Load the data\n",
    "data = pd.read_csv(\"cleaned_paper.csv\")\n",
    "print(\"Data loaded with shape:\", data.shape)\n",
    "\n",
    "# Define a simple slang/abbreviation mapping (expand as needed)\n",
    "slang_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omfg\": \"oh my freaking god\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fml\": \"fuck my life\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"af\": \"as fuck\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"bc\": \"because\",\n",
    "    \"b/c\": \"because\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"ya\": \"you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"wat\": \"what\",\n",
    "    \"wut\": \"what\",\n",
    "    \"ya'll\": \"you all\",\n",
    "    \"yall\": \"you all\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"nope\": \"no\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"nah\": \"no\",\n",
    "    \"bruh\": \"bro\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"sis\": \"sister\",\n",
    "    \"fam\": \"family\",\n",
    "    \"hbu\": \"how about you\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hf\": \"have fun\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ffs\": \"for fuck's sake\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_slang(text, slang_mapping):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in slang_mapping.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: slang_mapping[x.group()], text)\n",
    "\n",
    "def aggressive_clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emojis and non-text symbols\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Remove emoticons (a simple regex for common ones)\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|:\\s?D|:-D|;\\s?\\)|;-\\))', '', text)\n",
    "    \n",
    "    # Remove Reddit markdown artifacts (e.g., > quotes, spoilers marked by >!, etc.)\n",
    "    text = re.sub(r'>!.*?!<', '', text)\n",
    "    text = re.sub(r'>.*', '', text)\n",
    "    \n",
    "    # Remove emotionally ambiguous punctuation (e.g., ellipses, repeated punctuation)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'([!?]){2,}', r'\\1', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation (except those that might contribute to meaning)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Normalize slang and abbreviations\n",
    "    text = normalize_slang(text, slang_dict)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply aggressive cleaning to the 'text' column\n",
    "data['clean_text'] = data['text'].apply(aggressive_clean_text)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(data['clean_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example multilabel encoding:\n",
      "   depression  anxiety  OCD  PTSD  autism  eatingdisorders  adhd  bipolar  \\\n",
      "0           1        0    0     0       0                0     0        0   \n",
      "1           1        0    0     0       0                0     0        0   \n",
      "2           1        0    0     0       0                0     0        0   \n",
      "3           1        0    0     0       0                0     0        0   \n",
      "4           1        0    0     0       0                0     0        0   \n",
      "\n",
      "   schizophrenia  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Multilabel Handling\n",
    "# \n",
    "# We assume each Reddit post may belong to multiple mental health categories.\n",
    "# For this example, we create multilabels from the \"subreddit\" column.\n",
    "# In a real-world scenario, a post might have several labels (e.g., \"depression\" and \"anxiety\").\n",
    "# Here, if the CSV has a single label per post, we convert it to a list.\n",
    "# We then use MultiLabelBinarizer to convert the labels into a multi-hot encoding.\n",
    "\n",
    "# %%\n",
    "# Assume the \"subreddit\" column might contain comma-separated labels\n",
    "def split_labels(label_str):\n",
    "    # Split by comma and strip whitespace\n",
    "    return [lab.strip() for lab in label_str.split(',')]\n",
    "\n",
    "# Convert the subreddit column into a list of labels\n",
    "data['labels'] = data['subreddit'].apply(lambda x: split_labels(x))\n",
    "\n",
    "# Initialize the multilabel binarizer with the known list of disorders\n",
    "disorder_list = [\"depression\", \"anxiety\", \"OCD\", \"PTSD\", \"autism\", \"eatingdisorders\", \"adhd\", \"bipolar\", \"schizophrenia\"]\n",
    "mlb = MultiLabelBinarizer(classes=disorder_list)\n",
    "y = mlb.fit_transform(data['labels'])\n",
    "\n",
    "print(\"Example multilabel encoding:\")\n",
    "print(pd.DataFrame(y, columns=mlb.classes_).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Tokenization Using RoBERTa's Tokenizer\n",
    "# \n",
    "# We use the RoBERTa tokenizer from Hugging Face to tokenize our aggressively preprocessed text.\n",
    "# We pay attention to the maximum token length (configured via WandB config) and use truncation.\n",
    "\n",
    "# %%\n",
    "# Load RoBERTa tokenizer (you can choose a specific variant such as \"roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=config.max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # or use dynamic padding in DataLoader's collate_fn\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenize all posts (this can be optimized or done on the fly in a custom Dataset)\n",
    "data['tokenized'] = data['clean_text'].apply(lambda x: tokenize_function(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Create a Custom Dataset\n",
    "# \n",
    "# We define a PyTorch Dataset that yields input_ids, attention_mask, and multilabel targets for training.\n",
    " \n",
    "# %%\n",
    "class RedditMentalHealthDataset(Dataset):\n",
    "    def __init__(self, data_df, mlb):\n",
    "        self.data = data_df\n",
    "        self.mlb = mlb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Each tokenized item is a dict of tensors; we squeeze the batch dimension\n",
    "        tokenized_item = self.data.iloc[idx]['tokenized']\n",
    "        input_ids = tokenized_item['input_ids'].squeeze()  # shape: (max_length,)\n",
    "        attention_mask = tokenized_item['attention_mask'].squeeze()\n",
    "        labels = torch.tensor(mlb.transform([self.data.iloc[idx]['labels']])[0], dtype=torch.float32)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# Split the data into training and validation sets (e.g., 80/20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_dataset = RedditMentalHealthDataset(train_df, mlb)\n",
    "val_dataset = RedditMentalHealthDataset(val_df, mlb)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Model Architecture: RoBERTa Base with Custom Multilabel Classifier\n",
    "# \n",
    "# We use a pretrained RoBERTa as our encoder. A dropout layer is added between the encoder output and\n",
    "# a custom classifier head that produces one logit per label. We use sigmoid activation during inference.\n",
    "# We also freeze some lower RoBERTa layers during early training to help reduce GPU memory usage.\n",
    "\n",
    "# %%\n",
    "class RoBERTaMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, dropout_rate=config.dropout):\n",
    "        super(RoBERTaMultiLabelClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        # Load pretrained RoBERTa\n",
    "        self.config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "        self.roberta = AutoModel.from_pretrained(\"roberta-base\", config=self.config)\n",
    "        \n",
    "        # Optionally freeze lower layers (e.g., first 6 layers)\n",
    "        for name, param in self.roberta.named_parameters():\n",
    "            if \"encoder.layer\" in name:\n",
    "                layer_num = int(name.split(\".\")[2])\n",
    "                if layer_num < 6:  # freeze lower layers\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the last hidden state of the CLS token (index 0)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model and move it to GPU\n",
    "num_labels = len(mlb.classes_)\n",
    "model = RoBERTaMultiLabelClassifier(num_labels=num_labels).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.1079 | Val Macro F1: 0.8543\n",
      "Per-label precision: [0.81153716 0.87200492 0.92390646 0.92133009 0.93239817 0.96444059\n",
      " 0.94358974 0.84870389 0.87456647]\n",
      "Per-label recall: [0.74810822 0.76789901 0.8660836  0.82478219 0.81144286 0.94517637\n",
      " 0.87418913 0.74736611 0.74373259]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.81      0.75      0.78      9647\n",
      "        anxiety       0.87      0.77      0.82     11090\n",
      "            OCD       0.92      0.87      0.89      8804\n",
      "           PTSD       0.92      0.82      0.87      8264\n",
      "         autism       0.93      0.81      0.87      6799\n",
      "eatingdisorders       0.96      0.95      0.95      2353\n",
      "           adhd       0.94      0.87      0.91     10945\n",
      "        bipolar       0.85      0.75      0.79      9112\n",
      "  schizophrenia       0.87      0.74      0.80      6103\n",
      "\n",
      "      micro avg       0.89      0.80      0.85     73117\n",
      "      macro avg       0.90      0.81      0.85     73117\n",
      "   weighted avg       0.89      0.80      0.85     73117\n",
      "    samples avg       0.80      0.80      0.80     73117\n",
      "\n",
      "Best model saved at epoch 1 with Macro F1: 0.8543\n",
      "Epoch 2/5 | Loss: 0.0863 | Val Macro F1: 0.8588\n",
      "Per-label precision: [0.83161954 0.84669453 0.91161705 0.89038341 0.90132296 0.9499369\n",
      " 0.95385385 0.90272753 0.85348711]\n",
      "Per-label recall: [0.7377423  0.8037872  0.8868696  0.857091   0.8517429  0.95962601\n",
      " 0.87062586 0.71191835 0.7760118 ]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.83      0.74      0.78      9647\n",
      "        anxiety       0.85      0.80      0.82     11090\n",
      "            OCD       0.91      0.89      0.90      8804\n",
      "           PTSD       0.89      0.86      0.87      8264\n",
      "         autism       0.90      0.85      0.88      6799\n",
      "eatingdisorders       0.95      0.96      0.95      2353\n",
      "           adhd       0.95      0.87      0.91     10945\n",
      "        bipolar       0.90      0.71      0.80      9112\n",
      "  schizophrenia       0.85      0.78      0.81      6103\n",
      "\n",
      "      micro avg       0.89      0.82      0.85     73117\n",
      "      macro avg       0.89      0.83      0.86     73117\n",
      "   weighted avg       0.89      0.82      0.85     73117\n",
      "    samples avg       0.82      0.82      0.82     73117\n",
      "\n",
      "Best model saved at epoch 2 with Macro F1: 0.8588\n",
      "Epoch 3/5 | Loss: 0.0760 | Val Macro F1: 0.8615\n",
      "Per-label precision: [0.7901763  0.83180206 0.94580661 0.91973154 0.9300837  0.97953737\n",
      " 0.93545014 0.88617234 0.88579278]\n",
      "Per-label recall: [0.80377319 0.8245266  0.85438437 0.82913843 0.83350493 0.9358266\n",
      " 0.89904066 0.72794118 0.73963624]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.79      0.80      0.80      9647\n",
      "        anxiety       0.83      0.82      0.83     11090\n",
      "            OCD       0.95      0.85      0.90      8804\n",
      "           PTSD       0.92      0.83      0.87      8264\n",
      "         autism       0.93      0.83      0.88      6799\n",
      "eatingdisorders       0.98      0.94      0.96      2353\n",
      "           adhd       0.94      0.90      0.92     10945\n",
      "        bipolar       0.89      0.73      0.80      9112\n",
      "  schizophrenia       0.89      0.74      0.81      6103\n",
      "\n",
      "      micro avg       0.89      0.82      0.85     73117\n",
      "      macro avg       0.90      0.83      0.86     73117\n",
      "   weighted avg       0.89      0.82      0.85     73117\n",
      "    samples avg       0.82      0.82      0.82     73117\n",
      "\n",
      "Best model saved at epoch 3 with Macro F1: 0.8615\n",
      "Epoch 4/5 | Loss: 0.0670 | Val Macro F1: 0.8614\n",
      "Per-label precision: [0.80493801 0.8632108  0.94184592 0.89587302 0.8915244  0.98030439\n",
      " 0.91572316 0.85306828 0.84259749]\n",
      "Per-label recall: [0.7806572  0.78981064 0.85540663 0.85370281 0.86792175 0.93072673\n",
      " 0.91630882 0.7582309  0.7824021 ]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.80      0.78      0.79      9647\n",
      "        anxiety       0.86      0.79      0.82     11090\n",
      "            OCD       0.94      0.86      0.90      8804\n",
      "           PTSD       0.90      0.85      0.87      8264\n",
      "         autism       0.89      0.87      0.88      6799\n",
      "eatingdisorders       0.98      0.93      0.95      2353\n",
      "           adhd       0.92      0.92      0.92     10945\n",
      "        bipolar       0.85      0.76      0.80      9112\n",
      "  schizophrenia       0.84      0.78      0.81      6103\n",
      "\n",
      "      micro avg       0.88      0.83      0.85     73117\n",
      "      macro avg       0.89      0.84      0.86     73117\n",
      "   weighted avg       0.88      0.83      0.85     73117\n",
      "    samples avg       0.83      0.83      0.83     73117\n",
      "\n",
      "Epoch 5/5 | Loss: 0.0586 | Val Macro F1: 0.8590\n",
      "Per-label precision: [0.79165349 0.87917658 0.92069171 0.89441391 0.87501829 0.97127937\n",
      " 0.94876567 0.83688623 0.85376147]\n",
      "Per-label recall: [0.77868767 0.7663661  0.87687415 0.84668441 0.87939403 0.94857629\n",
      " 0.88487894 0.76690079 0.76241193]\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.79      0.78      0.79      9647\n",
      "        anxiety       0.88      0.77      0.82     11090\n",
      "            OCD       0.92      0.88      0.90      8804\n",
      "           PTSD       0.89      0.85      0.87      8264\n",
      "         autism       0.88      0.88      0.88      6799\n",
      "eatingdisorders       0.97      0.95      0.96      2353\n",
      "           adhd       0.95      0.88      0.92     10945\n",
      "        bipolar       0.84      0.77      0.80      9112\n",
      "  schizophrenia       0.85      0.76      0.81      6103\n",
      "\n",
      "      micro avg       0.88      0.82      0.85     73117\n",
      "      macro avg       0.89      0.83      0.86     73117\n",
      "   weighted avg       0.88      0.82      0.85     73117\n",
      "    samples avg       0.82      0.82      0.82     73117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Training Loop with Multilabel Loss and Logging\n",
    "# \n",
    "# We train the model using BCEWithLogitsLoss for multilabel classification.\n",
    "# We log metrics (macro F1, per-label precision/recall/F1) using WandB and save the best-performing model checkpoint.\n",
    "\n",
    "# %%\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "# For saving the best model\n",
    "best_macro_f1 = 0.0\n",
    "checkpoint_path = \"best_roberta_multilabel.pt\"\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"labels\"].to(\"cuda\")\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            # Apply sigmoid to get probabilities\n",
    "            probs = torch.sigmoid(logits)\n",
    "            # Use a threshold (e.g., 0.5) to obtain binary predictions\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    report = classification_report(all_labels, all_preds, target_names=mlb.classes_, zero_division=0)\n",
    "    return macro_f1, precision, recall, report\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config.epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    macro_f1, precision, recall, report = evaluate(model, val_loader)\n",
    "    \n",
    "    # Log metrics with WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Val Macro F1: {macro_f1:.4f}\")\n",
    "    print(\"Per-label precision:\", precision)\n",
    "    print(\"Per-label recall:\", recall)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Save best model based on macro F1\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Best model saved at epoch {epoch+1} with Macro F1: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['anxiety']\n",
      "Raw probabilities: [1.3170077e-01 6.7152125e-01 2.5383629e-02 2.2877686e-02 4.3020025e-02\n",
      " 9.2870541e-05 3.8182020e-02 2.5724376e-02 1.5826220e-02]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Inference and Demo\n",
    "# \n",
    "# This cell demonstrates how to preprocess a new Reddit post, tokenize it with RoBERTa’s tokenizer,\n",
    "# pass it through the model, and output the multilabel predictions. The model returns probabilities;\n",
    "# here we use a threshold of 0.5 for each label.\n",
    "\n",
    "# %%\n",
    "def predict_multilabel(text, model, tokenizer, mlb, threshold=0.5):\n",
    "    model.eval()\n",
    "    # Aggressive cleaning as before\n",
    "    cleaned_text = aggressive_clean_text(text)\n",
    "    tokenized = tokenizer(\n",
    "        cleaned_text,\n",
    "        max_length=config.max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).float().cpu().numpy()[0]\n",
    "    \n",
    "    # Map predictions to label names\n",
    "    predicted_labels = [label for label, flag in zip(mlb.classes_, preds) if flag == 1]\n",
    "    return predicted_labels, probs.cpu().numpy()[0]\n",
    "\n",
    "# Demo prediction\n",
    "sample_text = \"I'm constantly overwhelmed and anxious, yet sometimes I laugh it off as if nothing's wrong.\"\n",
    "predicted_labels, probs = predict_multilabel(sample_text, model, tokenizer, mlb)\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Raw probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-label Evaluation Metrics:\n",
      "\n",
      "Label: depression\n",
      "  Accuracy: 0.9438\n",
      "  Confusion Matrix:\n",
      "[[61493  1977]\n",
      " [ 2135  7512]]\n",
      "  Precision: 0.7917, Recall: 0.7787, F1 Score: 0.7851\n",
      "\n",
      "Label: anxiety\n",
      "  Accuracy: 0.9486\n",
      "  Confusion Matrix:\n",
      "[[60859  1168]\n",
      " [ 2591  8499]]\n",
      "  Precision: 0.8792, Recall: 0.7664, F1 Score: 0.8189\n",
      "\n",
      "Label: OCD\n",
      "  Accuracy: 0.9761\n",
      "  Confusion Matrix:\n",
      "[[63648   665]\n",
      " [ 1084  7720]]\n",
      "  Precision: 0.9207, Recall: 0.8769, F1 Score: 0.8982\n",
      "\n",
      "Label: PTSD\n",
      "  Accuracy: 0.9714\n",
      "  Confusion Matrix:\n",
      "[[64027   826]\n",
      " [ 1267  6997]]\n",
      "  Precision: 0.8944, Recall: 0.8467, F1 Score: 0.8699\n",
      "\n",
      "Label: autism\n",
      "  Accuracy: 0.9771\n",
      "  Confusion Matrix:\n",
      "[[65464   854]\n",
      " [  820  5979]]\n",
      "  Precision: 0.8750, Recall: 0.8794, F1 Score: 0.8772\n",
      "\n",
      "Label: eatingdisorders\n",
      "  Accuracy: 0.9974\n",
      "  Confusion Matrix:\n",
      "[[70698    66]\n",
      " [  121  2232]]\n",
      "  Precision: 0.9713, Recall: 0.9486, F1 Score: 0.9598\n",
      "\n",
      "Label: adhd\n",
      "  Accuracy: 0.9756\n",
      "  Confusion Matrix:\n",
      "[[61649   523]\n",
      " [ 1260  9685]]\n",
      "  Precision: 0.9488, Recall: 0.8849, F1 Score: 0.9157\n",
      "\n",
      "Label: bipolar\n",
      "  Accuracy: 0.9523\n",
      "  Confusion Matrix:\n",
      "[[62643  1362]\n",
      " [ 2124  6988]]\n",
      "  Precision: 0.8369, Recall: 0.7669, F1 Score: 0.8004\n",
      "\n",
      "Label: schizophrenia\n",
      "  Accuracy: 0.9693\n",
      "  Confusion Matrix:\n",
      "[[66217   797]\n",
      " [ 1450  4653]]\n",
      "  Precision: 0.8538, Recall: 0.7624, F1 Score: 0.8055\n",
      "\n",
      "Overall Subset Accuracy (Exact Match): 0.8229413132376875\n",
      "\n",
      "Detailed Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     depression       0.79      0.78      0.79      9647\n",
      "        anxiety       0.88      0.77      0.82     11090\n",
      "            OCD       0.92      0.88      0.90      8804\n",
      "           PTSD       0.89      0.85      0.87      8264\n",
      "         autism       0.88      0.88      0.88      6799\n",
      "eatingdisorders       0.97      0.95      0.96      2353\n",
      "           adhd       0.95      0.88      0.92     10945\n",
      "        bipolar       0.84      0.77      0.80      9112\n",
      "  schizophrenia       0.85      0.76      0.81      6103\n",
      "\n",
      "      micro avg       0.88      0.82      0.85     73117\n",
      "      macro avg       0.89      0.83      0.86     73117\n",
      "   weighted avg       0.88      0.82      0.85     73117\n",
      "    samples avg       0.82      0.82      0.82     73117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Detailed Evaluation: Per-label Accuracy, Confusion Matrix, and Additional Metrics\n",
    "\n",
    "# %% \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Evaluate the model on the validation set to gather predictions and true labels\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "# Print per-label evaluation metrics including accuracy and confusion matrix for each subreddit (label)\n",
    "print(\"Per-label Evaluation Metrics:\\n\")\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    y_true = all_labels[:, idx]\n",
    "    y_pred = all_preds[:, idx]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Output metrics for the label\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"  Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Compute overall subset accuracy (exact match ratio) and classification report\n",
    "# Note: Subset accuracy is strict and requires all label predictions to be correct for a sample.\n",
    "overall_subset_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Overall Subset Accuracy (Exact Match):\", overall_subset_accuracy)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=mlb.classes_, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
