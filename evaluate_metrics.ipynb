{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ec2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sahil\n",
      "[nltk_data]     Prusti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sahil\n",
      "[nltk_data]     Prusti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "# Aggressive text cleaning may require emoji support\n",
    "import emoji\n",
    "\n",
    "# For tokenization and normalization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Hugging Face Transformers for RoBERTa\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "# PyTorch and related modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn for multilabel metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training configuration\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "max_length = 128\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09f13f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with shape: (365582, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data Loading \n",
    "data = pd.read_csv(\"cleaned_paper.csv\")\n",
    "print(\"Data loaded with shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff5c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omfg\": \"oh my freaking god\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fml\": \"fuck my life\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"af\": \"as fuck\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"bc\": \"because\",\n",
    "    \"b/c\": \"because\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"ya\": \"you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"wat\": \"what\",\n",
    "    \"wut\": \"what\",\n",
    "    \"ya'll\": \"you all\",\n",
    "    \"yall\": \"you all\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"nope\": \"no\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"nah\": \"no\",\n",
    "    \"bruh\": \"bro\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"sis\": \"sister\",\n",
    "    \"fam\": \"family\",\n",
    "    \"hbu\": \"how about you\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hf\": \"have fun\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ffs\": \"for fuck's sake\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26abc05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    is hiring a life coach worth the money i am a ...\n",
      "1    i dont know whether im clinically depressed or...\n",
      "2    i always have big plans and ideas but i fuck u...\n",
      "3    why is everything going wrong i go to a boardi...\n",
      "4    cant tell if im depressed thanks in advance to...\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def normalize_slang(text, mapping):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in mapping) + r')\\b')\n",
    "    return pattern.sub(lambda m: mapping[m.group()], text)\n",
    "\n",
    "def aggressive_clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|:\\s?D|:-D|;\\s?\\)|;-\\))', '', text)\n",
    "    text = re.sub(r'>!.*?!<', '', text)\n",
    "    text = re.sub(r'>.*', '', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'([!?]){2,}', r'\\1', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = normalize_slang(text, slang_dict)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "data['clean_text'] = data['text'].apply(aggressive_clean_text)\n",
    "print(data['clean_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452ada55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   depression  anxiety  OCD  PTSD  autism  eatingdisorders  adhd  bipolar  \\\n",
      "0           1        0    0     0       0                0     0        0   \n",
      "1           1        0    0     0       0                0     0        0   \n",
      "2           1        0    0     0       0                0     0        0   \n",
      "3           1        0    0     0       0                0     0        0   \n",
      "4           1        0    0     0       0                0     0        0   \n",
      "\n",
      "   schizophrenia  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n"
     ]
    }
   ],
   "source": [
    "# Multilabel Handling\n",
    "def split_labels(label_str):\n",
    "    return [lab.strip() for lab in label_str.split(',')]\n",
    "\n",
    "data['labels'] = data['subreddit'].apply(split_labels)\n",
    "disorder_list = [\"depression\",\"anxiety\",\"OCD\",\"PTSD\",\"autism\",\"eatingdisorders\",\"adhd\",\"bipolar\",\"schizophrenia\"]\n",
    "mlb = MultiLabelBinarizer(classes=disorder_list)\n",
    "y = mlb.fit_transform(data['labels'])\n",
    "print(pd.DataFrame(y, columns=mlb.classes_).head())\n",
    "\n",
    "# Tokenization Using RoBERTa's Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "data['tokenized'] = data['clean_text'].apply(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6230cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Training on:\", device)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5994de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Custom Dataset and DataLoaders\n",
    "class RedditMentalHealthDataset(Dataset):\n",
    "    def __init__(self, df, mlb):\n",
    "        self.data = df\n",
    "        self.mlb = mlb\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        tok = self.data.iloc[idx]['tokenized']\n",
    "        input_ids = tok['input_ids'].squeeze()\n",
    "        attention_mask = tok['attention_mask'].squeeze()\n",
    "        labels = torch.tensor(self.mlb.transform([self.data.iloc[idx]['labels']])[0], dtype=torch.float32)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(RedditMentalHealthDataset(train_df, mlb), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(RedditMentalHealthDataset(val_df, mlb),   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63639a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture: RoBERTa Base with Custom Multilabel Classifier\n",
    "class RoBERTaMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        cfg = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "        self.roberta = AutoModel.from_pretrained(\"roberta-base\", config=cfg)\n",
    "        # freeze first 6 layers\n",
    "        for name, param in self.roberta.named_parameters():\n",
    "            if \"encoder.layer\" in name and int(name.split(\".\")[2]) < 6:\n",
    "                param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(cfg.hidden_size, num_labels)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RoBERTaMultiLabelClassifier(num_labels=len(mlb.classes_)).to(device)\n",
    "\n",
    "# Training Loop with Multilabel Loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "best_macro_f1 = 0.0\n",
    "checkpoint_path = \"best_roberta_multilabel.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654d2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall subset‑accuracy: 0.8834707398066645\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     depression     0.8489    0.8884    0.8682     48180\n",
      "        anxiety     0.8769    0.8982    0.8874     55067\n",
      "            OCD     0.9760    0.8862    0.9289     44111\n",
      "           PTSD     0.9734    0.8630    0.9149     41192\n",
      "         autism     0.9654    0.9094    0.9365     34003\n",
      "eatingdisorders     0.9873    0.9805    0.9839     11545\n",
      "           adhd     0.9714    0.9474    0.9593     54674\n",
      "        bipolar     0.9474    0.7803    0.8558     45800\n",
      "  schizophrenia     0.8964    0.8649    0.8804     31010\n",
      "\n",
      "      micro avg     0.9300    0.8849    0.9069    365582\n",
      "      macro avg     0.9381    0.8909    0.9128    365582\n",
      "   weighted avg     0.9324    0.8849    0.9068    365582\n",
      "    samples avg     0.8842    0.8849    0.8844    365582\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sahil Prusti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation on Full Dataset\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "full_loader = DataLoader(RedditMentalHealthDataset(data, mlb), batch_size=batch_size, shuffle=False)\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        logits = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "        proba = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (proba > 0.5).astype(int)\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(batch[\"labels\"].numpy())\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true = np.vstack(all_true)\n",
    "\n",
    "print(\"Overall subset‑accuracy:\", accuracy_score(all_true, all_preds))\n",
    "print(classification_report(all_true, all_preds, target_names=mlb.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aad855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall subset‑accuracy: 0.8835\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     depression     0.8489    0.8884    0.8682     48180\n",
      "        anxiety     0.8769    0.8982    0.8874     55067\n",
      "            OCD     0.9760    0.8862    0.9289     44111\n",
      "           PTSD     0.9734    0.8630    0.9149     41192\n",
      "         autism     0.9654    0.9094    0.9365     34003\n",
      "eatingdisorders     0.9873    0.9805    0.9839     11545\n",
      "           adhd     0.9714    0.9474    0.9593     54674\n",
      "        bipolar     0.9474    0.7803    0.8558     45800\n",
      "  schizophrenia     0.8964    0.8649    0.8804     31010\n",
      "\n",
      "      micro avg     0.9300    0.8849    0.9069    365582\n",
      "      macro avg     0.9381    0.8909    0.9128    365582\n",
      "   weighted avg     0.9324    0.8849    0.9068    365582\n",
      "    samples avg     0.8842    0.8849    0.8844    365582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Re‑instantiate your model and load the checkpoint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RoBERTaMultiLabelClassifier(num_labels=len(mlb.classes_)).to(device)\n",
    "model.load_state_dict(torch.load(\"best_roberta_multilabel.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2) Build a DataLoader over whatever split you want (e.g. all data or a hold‑out)\n",
    "full_dataset = RedditMentalHealthDataset(data, mlb)\n",
    "full_loader  = DataLoader(full_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 3) Run inference & collect preds/true labels\n",
    "all_preds = []\n",
    "all_true  = []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids     = batch[\"input_ids\"].to(device)\n",
    "        attention_mask= batch[\"attention_mask\"].to(device)\n",
    "        labels        = batch[\"labels\"].cpu().numpy()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds  = (probs > 0.5).astype(int)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_true .append(labels)\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true  = np.vstack(all_true)\n",
    "\n",
    "# 4) Compute overall “subset” accuracy and per‑label metrics\n",
    "overall_acc = accuracy_score(all_true, all_preds)  \n",
    "print(f\"Overall subset‑accuracy: {overall_acc:.4f}\\n\")\n",
    "\n",
    "print(classification_report(\n",
    "    all_true,\n",
    "    all_preds,\n",
    "    target_names=mlb.classes_,\n",
    "    zero_division=0,\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c1b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per‑label accuracy:\n",
      "depression: 0.9645\n",
      "anxiety: 0.9657\n",
      "OCD: 0.9836\n",
      "PTSD: 0.9819\n",
      "autism: 0.9885\n",
      "eatingdisorders: 0.9990\n",
      "adhd: 0.9880\n",
      "bipolar: 0.9671\n",
      "schizophrenia: 0.9801\n"
     ]
    }
   ],
   "source": [
    "# 6) Per‑label accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_per_label = {\n",
    "    label: accuracy_score(all_true[:, i], all_preds[:, i])\n",
    "    for i, label in enumerate(mlb.classes_)\n",
    "}\n",
    "\n",
    "print(\"Per‑label accuracy:\")\n",
    "for label, acc in accuracy_per_label.items():\n",
    "    print(f\"{label}: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
